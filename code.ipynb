{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU support) is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (GPU: NVIDIA GeForce RTX 3060 Laptop GPU)\n",
      "Epoch 1/30 | Train Loss: 336.272 | Val Loss: 274.681\n",
      "Epoch 2/30 | Train Loss: 265.141 | Val Loss: 259.823\n",
      "Epoch 3/30 | Train Loss: 256.102 | Val Loss: 253.439\n",
      "Epoch 4/30 | Train Loss: 250.792 | Val Loss: 248.937\n",
      "Epoch 5/30 | Train Loss: 247.891 | Val Loss: 247.142\n",
      "Epoch 6/30 | Train Loss: 246.000 | Val Loss: 245.882\n",
      "Epoch 7/30 | Train Loss: 244.863 | Val Loss: 244.214\n",
      "Epoch 8/30 | Train Loss: 243.779 | Val Loss: 243.652\n",
      "Epoch 9/30 | Train Loss: 243.065 | Val Loss: 242.875\n",
      "Epoch 10/30 | Train Loss: 242.350 | Val Loss: 242.154\n",
      "Epoch 11/30 | Train Loss: 241.756 | Val Loss: 241.880\n",
      "Epoch 12/30 | Train Loss: 241.251 | Val Loss: 241.523\n",
      "Epoch 13/30 | Train Loss: 240.893 | Val Loss: 240.819\n",
      "Epoch 14/30 | Train Loss: 240.523 | Val Loss: 240.619\n",
      "Epoch 15/30 | Train Loss: 240.089 | Val Loss: 240.307\n",
      "Epoch 16/30 | Train Loss: 239.788 | Val Loss: 239.946\n",
      "Epoch 17/30 | Train Loss: 239.598 | Val Loss: 240.027\n",
      "Epoch 18/30 | Train Loss: 239.410 | Val Loss: 239.831\n",
      "Epoch 19/30 | Train Loss: 239.146 | Val Loss: 239.339\n",
      "Epoch 20/30 | Train Loss: 238.887 | Val Loss: 239.183\n",
      "Epoch 21/30 | Train Loss: 238.706 | Val Loss: 239.165\n",
      "Epoch 22/30 | Train Loss: 238.558 | Val Loss: 238.783\n",
      "Epoch 23/30 | Train Loss: 238.385 | Val Loss: 238.899\n",
      "Epoch 24/30 | Train Loss: 238.251 | Val Loss: 238.754\n",
      "Epoch 25/30 | Train Loss: 238.118 | Val Loss: 238.538\n",
      "Epoch 26/30 | Train Loss: 238.034 | Val Loss: 238.483\n",
      "Epoch 27/30 | Train Loss: 237.890 | Val Loss: 238.346\n",
      "Epoch 28/30 | Train Loss: 237.755 | Val Loss: 238.083\n",
      "Epoch 29/30 | Train Loss: 237.604 | Val Loss: 238.125\n",
      "Epoch 30/30 | Train Loss: 237.514 | Val Loss: 238.197\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "# -------------------- GPU Configuration -------------------- \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} (GPU: {torch.cuda.get_device_name(0)})\" if torch.cuda.is_available() else \"Using CPU\")\n",
    "\n",
    "# -------------------- Model Definition --------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2*latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64*7*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        return mu, log_var\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)  # Now explicitly defined\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var\n",
    "\n",
    "# -------------------- Data Loaders with GPU Pinning --------------------\n",
    "train_dataset = FashionMNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = FashionMNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Train/Validation split\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 256  # Larger batch size for GPU efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         pin_memory=True, num_workers=4)  # GPU-optimized loading\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                       pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------- Training Setup --------------------\n",
    "model = VAE(latent_dim=20).to(device)  # Entire model moved to GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum') / x.size(0)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / x.size(0)\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "# -------------------- GPU-Optimized Training Loop --------------------\n",
    "epochs = 30\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss, _, _ = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            recon, mu, log_var = model(data)\n",
    "            loss, _, _ = loss_function(recon, data, mu, log_var)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Save losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save samples every epoch\n",
    "    with torch.no_grad():\n",
    "        # Reconstruction samples\n",
    "        test_images, _ = next(iter(test_loader))\n",
    "        test_images = test_images.to(device)\n",
    "        recon_images, _, _ = model(test_images)\n",
    "        \n",
    "        # Concatenate original and reconstructed\n",
    "        comparison = torch.cat([test_images[:8], recon_images[:8]]).cpu()\n",
    "        save_image(comparison, f'samples/recon_epoch_{epoch+1}.png', nrow=8)  # Now works\n",
    "        \n",
    "        # Generate new images\n",
    "        z = torch.randn(64, model.latent_dim).to(device)\n",
    "        generated = model.decode(z).cpu()\n",
    "        save_image(generated, f'samples/generated_epoch_{epoch+1}.png', nrow=8)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
    "\n",
    "# -------------------- GPU-Specific Performance Tips --------------------\n",
    "# 1. Use mixed-precision training (uncomment below)\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "# scaler = GradScaler()\n",
    "# Inside training loop:\n",
    "# with autocast():\n",
    "#     recon_batch, mu, log_var = model(data)\n",
    "#     loss, bce, kld = loss_function(...)\n",
    "# scaler.scale(loss).backward()\n",
    "# scaler.step(optimizer)\n",
    "# scaler.update()\n",
    "\n",
    "# 2. Use torch.backends.cudnn.benchmark = True (if input sizes don't vary)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# -------------------- Visualization (Move tensors to CPU) --------------------\n",
    "# Reconstructions\n",
    "model.eval()\n",
    "test_images, _ = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    test_images = test_images.to(device)\n",
    "    recon_images, _, _ = model(test_images)\n",
    "    test_images = test_images.cpu()  # Move back to CPU for plotting\n",
    "    recon_images = recon_images.cpu()\n",
    "\n",
    "# Latent Space Visualization (Partial example)\n",
    "latent_vectors = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data = data.to(device)\n",
    "        mu, _ = model.encode(data)\n",
    "        latent_vectors.append(mu.cpu())  # Move to CPU for sklearn\n",
    "        labels.append(label)\n",
    "latent_vectors = torch.cat(latent_vectors, dim=0).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
